{
    "filename": "Module 1 Notes (1).pdf",
    "file_type": "pyq",
    "extracted_at": "20251108_163425",
    "text_length": 58840,
    "text": "--- Page 1 ---\nModule 1: Natural Language Processing\nComponents - Basics of Linguistics and Probability and Statistics – Words-\nTokenization-Morphology:\nInflectional Morphology - Derivational Morphology. Finite-State Morphological \nParsing - Porter Stemmer.\nI. Introduction\nLanguage is a method of communication with the help of which we can speak, read and \nwrite. For example, we think, we make decisions, plans and more in natural language; \nprecisely, in words. However, the big question that confronts us in this AI era is that can we \ncommunicate in a similar manner with computers. In other words, can human beings \ncommunicate with computers in their natural language? It is a challenge for us to develop \nNLP applications because computers need structured data, but human speech is unstructured \nand often ambiguous in nature.\nIn this sense, we can say that Natural Language Processing (NLP) is the sub-field of \nComputer Science especially Artificial Intelligence (AI) that is concerned about enabling \ncomputers to understand and process human language. Technically, the main task of NLP \nwould be to program computers for analyzing and processing huge amount of natural \nlanguage data.\nNLP is a part of Computer Science, Human language,  and Artificial Intelligence . It is the \ntechnology that is used by machines to understand, analyze, manipulate, and interpret human \nlanguages. It helps developers to organize knowledge for performing tasks such as \ntranslation, automatic summarization, Named Entity Recognition (NER), speech \nrecognition, relationship extraction,  and topic segmentation .\nLanguage is a crucial component for human lives and also the most fundamental aspect of \nour behavior. We can experience it in mainly two forms - written and spoken. In the written \n\n--- Page 2 ---\nform, it is a way to pass our knowledge from one generation to the next. In the spoken form, \nit is the primary medium for human beings to coordinate with each other in their day-to-day \nbehavior. Language is studied in various academic disciplines. Each discipline comes with its \nown set of problems and a set of solution to address those.\nConsider the following table to understand this −\nDiscipline Problems Tools\nLinguistsHow phrases and sentences \ncan be formed with words?\nWhat curbs the possible \nmeaning for a sentence?Intuitions about well-formedness and \nmeaning.\nMathematical model of structure. For \nexample, model theoretic semantics, \nformal language theory.\nPsycholinguistsHow human beings can \nidentify the structure of \nsentences?\nHow the meaning of words \ncan be identified?\nWhen does understanding \ntake place?Experimental techniques mainly for \nmeasuring the performance of human \nbeings.\nStatistical analysis of observations.\nPhilosophersHow do words and sentences \nacquire the meaning?\nHow the objects are \nidentified by the words?\nWhat is meaning?Natural language argumentation by using \nintuition.\nMathematical models like logic and model \ntheory.\nComputational \nLinguistsHow can we identify the \nstructure of a sentence?\nHow knowledge and \nreasoning can be modeled?\nHow we can use language to \naccomplish specific tasks?Algorithms\nData structures\nFormal models of representation and \nreasoning.\nAI techniques like search & representation \nmethods.\nHistory of NLP\nNatural Language Processing started in 1950 When Alan Mathison Turing  published an \narticle in the name Computing Machinery and Intelligence.  It is based on Artificial \nintelligence. It talks about automatic interpretation and generation of natural language. As the \ntechnology evolved, different approaches have come to deal with NLP tasks.\nHeuristics-Based NLP:  This is the initial approach of NLP. It is based on defined \nrules. Which comes from domain knowledge and expertise. Example: regex\nStatistical Machine learning-based NLP:  It is based on statistical rules and machine \nlearning algorithms. In this approach, algorithms are applied to the data and learned \nfrom the data, and applied to various tasks. Examples: Naive Bayes, support vector \nmachine (SVM), hidden Markov model (HMM), etc.\nNeural Network-based NLP: This is the latest approach that comes with the \nevaluation of neural network-based learning, known as Deep learning. It provides \ngood accuracy, but it is a very data-hungry and time-consuming approach. It requires \nhigh computational power to train the model. Furthermore, it is based on neural \n\n--- Page 3 ---\nnetwork architecture. Examples: Recurrent neural networks (RNNs), Long short-term \nmemory networks (LSTMs), Convolutional neural networks (CNNs), Transformers, \netc.\nAdvantages of NLP\nNLP helps users to ask questions about any subject and get a direct response within \nseconds.\nNLP offers exact answers to the question means it does not offer unnecessary and \nunwanted information.\nNLP helps computers to communicate with humans in their languages.\nIt is very time efficient.\nMost of the companies use NLP to improve the efficiency of documentation \nprocesses, accuracy of documentation, and identify the information from large \ndatabases.\nDisadvantages of NLP\nA list of disadvantages of NLP is given below:\nNLP may not show context.\nNLP is unpredictable\nNLP may require more keystrokes.\nNLP is unable to adapt to the new domain, and it has a limited function that's why \nNLP is built for a single and specific task only. \nComponents of NLP\nThere are two components of Natural Language Processing:\nNatural Language Understanding\nNatural Language Generation\n1. Natural Language Understanding (NLU)\nNatural Language Understanding (NLU) helps the machine to understand and analyse human \nlanguage by extracting the metadata from content such as concepts, entities, keywords, \nemotion, relations, and semantic roles.\nNLU mainly used in Business applications to understand the customer's problem in both \nspoken and written language.\nNLU involves the following tasks -\nIt is used to map the given input into useful representation.\nIt is used to analyze different aspects of the language.\n2. Natural Language Generation (NLG)\nNatural Language Generation (NLG) acts as a translator that converts the computerized data \ninto natural language representation. It mainly involves Text planning, Sentence planning, \nand Text Realization.\nText planning  − It includes retrieving the relevant content from knowledge base.\nSentence planning  − It includes choosing required words, forming meaningful \nphrases, setting tone of the sentence.\nText Realization  − It is mapping sentence plan into sentence structure.\n\n--- Page 4 ---\nDifference between NLU and NLG\nNLU NLG\nNLU is the process of reading and \ninterpreting language.NLG is the process of writing or generating \nlanguage.\nIt produces non-linguistic outputs from \nnatural language inputs.It produces constructing natural language outputs \nfrom non-linguistic inputs.\nDifficulties in NLU\nNL has an extremely rich form and structure.\nIt is very ambiguous. There can be different levels of ambiguity −\nLexical ambiguity  − It is at very primitive level such as word-level.\nFor example, treating the word “board” as noun or verb?\nSyntax Level ambiguity  − A sentence can be parsed in different ways.\nFor example, “He lifted the beetle with red cap.” − Did he use cap to lift the beetle or he \nlifted a beetle that had red cap?\nReferential ambiguity  − Referring to something using pronouns. For example, Rima \nwent to Gauri. She said, “I am tired.” − Exactly who is tired?\nOne input can mean different meanings.\nMany inputs can mean the same thing.\nApplications of NLP\nThere are the following applications of NLP -\n1. Question Answering\nQuestion Answering focuses on building systems that automatically answer the questions \nasked by humans in a natural language.\n2. Spam Detection\nSpam detection is used to detect unwanted e-mails getting to a user's inbox\n3. Sentiment Analysis\nSentiment Analysis is also known as opinion mining . It is used on the web to analyse the \nattitude, behaviour, and emotional state of the sender. This application is implemented \nthrough a combination of NLP (Natural Language Processing) and statistics by assigning the \nvalues to the text (positive, negative, or natural), identify the mood of the context (happy, sad, \nangry, etc.)\n\n--- Page 5 ---\n4. Machine Translation\nMachine translation is used to translate text or speech from one natural language to another \nnatural language.\n5. Spelling correction\nMicrosoft Corporation provides word processor software like MS-word, PowerPoint for the \nspelling correction.\n6. Speech Recognition\nSpeech recognition is used for converting spoken words into text. It is used in applications, \nsuch as mobile, home automation, video recovery, dictating to Microsoft Word, voice \nbiometrics, voice user interface, and so on.\n7. Chatbot\nImplementing the Chatbot is one of the important applications of NLP. It is used by many \ncompanies to provide the customer's chat services.\n8. Information extraction\nInformation extraction is one of the most important applications of NLP. It is used for \nextracting structured information from unstructured or semi-structured machine-readable \ndocuments.\n9. Natural Language Understanding (NLU)\nIt converts a large set of text into more formal representations such as first-order logic \nstructures that are easier for the computer programs to manipulate notations of the natural \nlanguage processing.\nBasics of Linguistics:\n1. Phonetics and Phonology\nPhonetics  deals with the physical sounds of speech and how they are produced and \nperceived.\nPhonology  focuses on how sounds function within a particular language.\nRelevance in NLP:\noUsed in speech recognition  and speech synthesis  systems.\noHelps in analyzing the sound structure for tasks like text-to-speech  and voice \nassistants .\n2. Morphology\nMorphology  is the study of the structure of words and how they are formed from \nmorphemes, which are the smallest units of meaning.\n\n--- Page 6 ---\noInflectional Morphology : Modifies a word to express different grammatical \ncategories (e.g., tense, number).\noDerivational Morphology : Creates new words by adding prefixes or suffixes \n(e.g., happy  → unhappy ).\nRelevance in NLP:\noUsed in tokenization  (breaking text into individual words or subwords) and \nmorphological analysis .\noImportant for building stemming  and lemmatization  algorithms, which are \nused to normalize text.\noHelps in understanding word formation for language models  and machine \ntranslation .\n3. Syntax\nSyntax  is the study of how words combine to form sentences, and the rules governing \nsentence structure.\nRelevance in NLP:\noCrucial for parsing  and understanding the grammatical structure of sentences.\noUsed in dependency parsing  and constituency parsing  to break down a \nsentence into its grammatical components.\noImportant for machine translation , question answering systems , and text \ngeneration .\n4. Semantics\nSemantics  is concerned with the meaning of words, phrases, and sentences.\noLexical Semantics : Meaning of individual words.\noCompositional Semantics : Meaning of phrases and sentences as a whole.\nRelevance in NLP:\noHelps in building systems that understand the meaning of text, such as \nsemantic analysis , sentiment analysis , and machine translation .\noImportant for word embeddings  like Word2Vec or GloVe, which capture \nsemantic relationships between words.\noKey in natural language understanding (NLU)  tasks like information \nretrieval  and text classification .\n5. Pragmatics\nPragmatics  deals with how context influences the interpretation of meaning.\noFor example, understanding the intent behind a question, sarcasm, or \npoliteness.\nRelevance in NLP:\noImportant for dialogue systems , chatbots , and context-aware applications .\noHelps in improving text generation  and understanding user intent  in \nconversational agents .\n6. Discourse\nDiscourse analysis  looks at how sentences are structured in larger texts or \nconversations to create meaning.\nRelevance in NLP:\noUsed in building coherent text generation  systems like story generation  or \nsummarization .\n\n--- Page 7 ---\noImportant for dialogue management  and ensuring that interactions in \nchatbots and virtual assistants are contextually consistent.\n7. Pragmatic and Sociolinguistics\nFocuses on how language is used in different social contexts, such as formal versus \ninformal speech or how dialects influence communication.\nRelevance in NLP:\noImportant in sentiment analysis , emotion detection , and understanding social \nmedia language .\noHelps in developing models that adapt to different speaking or writing styles, \ntones, or regional variations.\nApplications of Linguistics in NLP\n1.Speech Recognition : Phonetics and phonology help in mapping speech sounds to \nwords.\n2.Machine Translation : Syntax, semantics, and morphology help translate text \nbetween languages.\n3.Sentiment Analysis : Semantics and pragmatics allow models to infer the sentiment \nbehind text.\n4.Chatbots and Conversational AI: Syntax, semantics, and pragmatics work together \nto help chatbots understand and respond naturally.\nLet us dive into the details of the above concepts.\nPhonetics and Phonology in Linguistics\n1. Phonetics\nPhonetics  is the study of the physical sounds of human speech. It focuses on how speech \nsounds are produced, transmitted, and received. Phonetics deals with the articulation (how \nsounds are made using the mouth, tongue, and vocal cords) and the acoustic properties of \nsounds (such as pitch, loudness, and duration). It is concerned with describing all sounds in a \nlanguage.\nExample : In the word bat, the phonetic sounds (or phones) can be represented as:\no/b/ (a voiced bilabial stop, where the vocal cords vibrate as the lips come \ntogether)\no/æ/ (a low-front vowel sound, as in cat or hat)\no/t/ (a voiceless alveolar stop, where the tongue touches the ridge behind the \nupper front teeth).\nEach sound can be represented phonetically using the International Phonetic Alphabet \n(IPA) , which provides a unique symbol for each distinct sound in human speech.\n2. Phonology\nPhonology , on the other hand, is the study of how sounds function within a particular \nlanguage or languages. It focuses on how sounds are organized in the mind, how they pattern, \nand how they change in different linguistic environments. Phonology deals with phonemes , \nwhich are the smallest units of sound that can change the meaning of a word.\n\n--- Page 8 ---\nExample : In English, the sounds /p/ and /b/ are two different phonemes . This means \nthat if you change the /p/ sound in pat to a /b/, the word becomes bat, which has a \ncompletely different meaning.\no/p/ and /b/ are thus contrastive  in English because replacing one with the \nother results in a different word.\noHowever, in some languages, like Arabic, the difference between these sounds \nmay not change meaning, and they could be considered variants (or \nallophones ) of the same phoneme.\nPhonetics vs. Phonology Example\nLet’s consider the words \"tap\"  and \"pat\"  in English:\nPhonetic analysis  focuses on the exact sounds (phones) in these words.\no\"Tap\" can be broken down into the phonetic symbols: /t/ /æ/ /p/\no\"Pat\" can be broken down into the phonetic symbols: /p/ /æ/ /t/\nThe sounds /t/ and /p/ are articulated differently but can be described using phonetic symbols \nregardless of their language.\nPhonological analysis  examines the role of these sounds (phonemes) within the \nsystem of the English language.\noIn English, /t/ and /p/ are separate phonemes because replacing /t/ with /p/ (or \nvice versa) changes the meaning of the word.\noPhonology also considers rules about how phonemes behave in different \ncontexts, such as how the /t/ in \"tap\" might sound different from the /t/ in \n\"butter\" (where it can become a \"flap\" sound like /ɾ/).\nPhonetic vs. Phonological Focus:\nPhonetics  asks: \"How are these sounds physically made and what are their acoustic \nproperties?\"\noExample: Describing the /t/ sound in tap as a voiceless alveolar stop.\nPhonology  asks: \"How do these sounds function in the language, and how do they \ninteract with other sounds?\"\noExample: In English, /t/ and /p/ are different phonemes, but in some \nlanguages, they might not be distinct.\nBoth concepts are crucial in speech recognition  and speech synthesis  in NLP, as \nunderstanding the physical properties of sounds (phonetics) and how they function in \nlanguage (phonology) helps systems interpret and generate human speech more effectively.\nMorphology in Linguistics\nMorphology  is the branch of linguistics that studies the structure and formation of words. It \nfocuses on morphemes , which are the smallest meaningful units of language. Morphemes \ncan be:\nFree morphemes : Can stand alone as words (e.g., book , run).\nBound morphemes : Cannot stand alone and must attach to other morphemes (e.g., -s, \nun-).\nMorphology helps in understanding how words are created and modified, which is crucial for \ntasks like tokenization , stemming , and lemmatization  in Natural Language Processing \n(NLP).\nTypes of Morphology\n\n--- Page 9 ---\n1.Inflectional Morphology\noDeals with changes in a word to express grammatical relationships without \nchanging the word's core meaning.\noExamples:\nPlural: cat → cats (adding -s to indicate more than one)\nPast tense: play → played  (adding -ed to show past tense)\nComparative: fast → faster  (adding -er to compare)\n2.Derivational Morphology\noInvolves creating a new word with a new meaning by adding prefixes or \nsuffixes to a base word. This often changes the word's grammatical category.\noExamples:\nVerb to noun: teach  → teacher  (adding -er to form a noun)\nAdjective to adverb: happy  → happily  (adding -ly to form an adverb)\nPrefixes: unhappy  (adding un- to change the meaning to the opposite)\nExamples of Morphemes\nIn the word \"unhappiness\" :\noun- (prefix, bound morpheme) means \"not\"\nohappy  (root word, free morpheme) is the base meaning\no-ness  (suffix, bound morpheme) turns the adjective into a noun\nIn this example, we can see both derivational morphology (prefix un- and suffix -ness ).\nMorphological Parsing\nMorphological parsing is the process of breaking down a word into its morphemes to \nunderstand its structure and meaning.\nExample:\nWord: \"rewritten\"\nore- (prefix, means \"again\")\nowrite  (root, meaning \"to write\")\no-en (suffix, participle marker indicating passive or past participle)\nSample Questions and Answers on Morphology\n1. Identify Morphemes in the Given Words:\nQuestion : Break down the following words into their morphemes:\n1.Unbelievable\n2.Plays\n3.Irreplaceable\nAnswer :\n1.Unbelievable  → un- (prefix, bound) + believe  (root, free) + -able  (suffix, \nbound)\n2.Plays  → play (root, free) + -s (suffix, bound, indicating plural)\n3.Irreplaceable  → ir- (prefix, bound) + replace  (root, free) + -able  (suffix, \nbound)\n2. Classify the Following Words as Inflectional or Derivational:\nQuestion : Determine whether the morphological changes in the following words are \ninflectional or derivational:\n1.Cats\n\n--- Page 10 ---\n2.Teacher\n3.Happier\n4.Unhappiness\nAnswer :\n1.Cats  → Inflectional (plural form of cat)\n2.Teacher  → Derivational (derived from teach  by adding -er to create a noun)\n3.Happier  → Inflectional (comparative form of happy )\n4.Unhappiness  → Derivational (derived from happy  with prefix un- and suffix -\nness)\n3. Word Formation:\nQuestion : Create new words using the given root and the appropriate morphemes.\n1.Root: hope  → Add a derivational morpheme to make a noun.\n2.Root: run → Add an inflectional morpheme to show past tense.\nAnswer :\n1.Hope  → hopeful  (Adding -ful to make an adjective) or hopeless  (Adding -less \nto form another adjective with a negative meaning).\n2.Run → ran (Past tense, irregular form).\n4. Stemming and Lemmatization:\nQuestion : Identify the root for the following inflected words:\n1.Playing\n2.Quickest\n3.Unhappily\nAnswer :\n1.Playing  → Root: play\n2.Quickest  → Root: quick\n3.Unhappily  → Root: happy\n5. True/False Questions:\nQuestion : Indicate whether the following statements are true or false.\n1.Inflectional morphemes change the meaning and part of speech of a word.\n2.The morpheme -ed in walked  is an example of derivational morphology.\nAnswer :\n1.False (Inflectional morphemes change grammatical aspects like tense or \nnumber but don't change the core meaning or part of speech).\n2.False (The -ed suffix is an example of inflectional morphology, showing past \ntense).\nIn NLP, morphological analysis helps with tasks like:\nTokenization : Breaking down text into meaningful units.\nStemming : Reducing words to their root forms (e.g., running  → run).\nLemmatization : Returning words to their dictionary form based on context (e.g., was \n→ be).\nUnderstanding morphology is crucial for building accurate models for tasks like machine \ntranslation , information retrieval , and text generation .\nSyntax in Linguistics\n\n--- Page 11 ---\nSyntax  is the set of rules that govern how words are arranged to form phrases, clauses, and \nsentences in a language. It dictates the structure of a sentence and ensures that words are in \nthe correct order so the sentence is grammatically correct and makes sense.\nSyntax also defines the relationships between words in a sentence, such as subject, object, \nand verb, and how they interact to convey meaning.\nKey Concepts in Syntax:\n1.Word Order : The most basic rule of syntax is the order of words in a sentence. \nDifferent languages follow different word orders. In English, the most common word \norder is Subject-Verb-Object (SVO) .\n2.Sentence Structure : Sentences can be simple (one clause) or complex (multiple \nclauses), but all sentences must follow syntactic rules to be grammatically correct.\nExample of Syntax:\nConsider the sentence:\n\"The cat chased the mouse.\"\nSubject : The cat\nVerb : chased\nObject : the mouse\nThis sentence follows the basic SVO  (Subject-Verb-Object) word order in English. The \nsubject ( the cat ) comes before the verb ( chased ), and the object ( the mouse ) follows the verb.\nSyntactically Incorrect Example:\n\"Chased the cat the mouse.\"\nThis sentence doesn't follow the correct word order in English, so it’s syntactically incorrect \neven though it contains the same words as the correct sentence.\nTypes of Sentences Based on Syntax:\n1.Simple Sentence : Contains one independent clause.\noExample: \"The dog barks.\"\n2.Compound Sentence : Contains two or more independent clauses joined by a \nconjunction.\noExample: \"The dog barks, and the cat runs.\"\n3.Complex Sentence : Contains one independent clause and one or more dependent \nclauses.\noExample: \"The dog barks when the cat runs.\"\nQuestions and Answers on Syntax:\nQuestion 1: Identify the syntactically correct sentence:\n1.\"The man the car drives.\"\n2.\"The man drives the car.\"\nAnswer : Sentence 2 is syntactically correct because it follows the proper SVO word order.\nQuestion 2: Correct the syntactically incorrect sentence:\n\"Playing the children are in the park.\"\nAnswer : \"The children are playing in the park.\"\nThis correction places the subject ( the children ) before the verb ( are playing ), following \nproper syntactic rules.\n\n--- Page 12 ---\nSyntax in NLP:\nIn Natural Language Processing, understanding syntax is essential for parsing sentences, \nenabling machines to determine the structure of a sentence. This helps in tasks like machine \ntranslation, question answering, and text generation.\nFor example, a syntactic parser will identify the subject, verb, and object in a sentence to \nunderstand its grammatical structure and ensure that translations or responses are coherent.\nSemantics in Linguistics\nSemantics  is the study of meaning in language. It focuses on the interpretation of words, \nphrases, and sentences, and how meaning is conveyed and understood. While syntax is \nconcerned with the structure of sentences, semantics  deals with the meaning behind those \nstructures.\nSemantics plays a crucial role in understanding how words relate to each other and how \nmeaning can change based on word choice or context.\nKey Concepts in Semantics:\n1.Lexical Semantics : The meaning of individual words and the relationships between \nthem. This includes synonyms (words with similar meanings), antonyms (words with \nopposite meanings), and homonyms (words that are spelled or pronounced the same \nbut have different meanings).\n2.Compositional Semantics : How the meaning of individual words combines to form \nthe meaning of a larger expression, such as a phrase or sentence. This is often referred \nto as the \"principle of compositionality,\" which states that the meaning of a sentence \nis determined by the meanings of its parts and the rules used to combine them.\n3.Ambiguity : Sentences or phrases can have multiple meanings. Semantic analysis \nhelps distinguish between these meanings based on context.\noLexical Ambiguity : When a word has multiple meanings (e.g., bank  can refer \nto a financial institution or the side of a river).\noStructural Ambiguity : When the structure of a sentence can lead to different \ninterpretations (e.g., \"I saw the man with the telescope\"  could mean either that \nthe speaker used a telescope or the man had a telescope).\nExample of Semantics:\nConsider the sentence:\n\"The bank is on the river.\"\nLexical Semantics : The word bank  here can have two different meanings:\noA financial institution.\noThe side of a river.\nBased on context, we infer that in this case, bank  refers to the land beside the river, not a \nplace where you store money.\nCompositional Semantics Example:\nConsider the sentence:\n\"John kicked the ball.\"\n\n--- Page 13 ---\nWord Meaning :\noJohn : a proper noun, referring to a specific person.\nokicked : an action involving hitting something with the foot.\nothe ball : a round object.\nCompositional Meaning : The sentence means that a person named John performed \nthe action of kicking a round object.\nSemantically Incorrect Example:\nSyntactically Correct but Semantically Incorrect : \"Colorless green ideas sleep \nfuriously.\"\noThe sentence is grammatically correct, but it doesn't make sense semantically. \nThe meaning of the words doesn’t logically fit together, as something cannot \nbe both \"colorless\" and \"green,\" and ideas cannot \"sleep.\"\nQuestions and Answers on Semantics:\nQuestion 1: Which of the following sentences is semantically incorrect?\n1.\"The sun rises in the east.\"\n2.\"The chair laughed at the joke.\"\nAnswer : Sentence 2 is semantically incorrect. A chair, being an inanimate object, cannot \n\"laugh.\"\nQuestion 2: What is the meaning of the word bank  in the sentence: \"He walked along the \nbank.\" ?\nAnswer : In this sentence, bank  refers to the land beside the river (riverbank), not a financial \ninstitution.\nSemantics in NLP:\nIn Natural Language Processing, understanding semantics is crucial for tasks like machine \ntranslation , sentiment analysis , and question answering . NLP systems need to understand \nthe meaning behind the text, not just its structure.\nFor example:\nIn machine translation , semantic analysis ensures that words are translated into their \ncorrect meanings based on context (e.g., translating bank  correctly based on whether \nit refers to a financial institution or a riverbank).\nIn sentiment analysis , semantics helps determine whether a sentence expresses \npositive, negative, or neutral sentiment based on the meanings of the words.\nPragmatics in Linguistics\nPragmatics  is the branch of linguistics that studies how context influences the interpretation \nof meaning in communication. It focuses on how speakers use language in real-world \nsituations and how listeners interpret it beyond just the literal meaning of words.\nUnlike semantics , which deals with the literal meanings of words and sentences, pragmatics \ntakes into account factors like:\nThe speaker's intention.\nThe listener's interpretation.\nThe social and physical context of the conversation.\n\n--- Page 14 ---\nPragmatics explains how meaning can change depending on context, tone, and other external \nfactors, including shared knowledge between the speaker and listener.\nKey Concepts in Pragmatics:\n1.Speech Acts :\noUtterances can function as actions. For example, when someone says, \"I \napologize,\"  they are not just stating a fact, they are performing the act of \napologizing.\noTypes of speech acts include requests , promises , apologies , commands , etc.\n2.Context :\noPhysical context : Where and when the communication takes place.\noLinguistic context : What has been said before in the conversation.\noSocial context : The relationship between the speaker and the listener.\n3.Deixis :\noWords like this, that, here, there , I, you depend on the context to convey \nmeaning. For example, the meaning of \"here\"  depends on where the speaker is \nlocated.\n4.Implicature :\noWhen a speaker implies something without directly stating it. For instance, \n\"It’s cold in here,\"  could imply a request to close the window, even though \nthat is not directly said.\nExamples of Pragmatics:\n1. Same Sentence, Different Meanings Based on Context:\nSentence: \"Can you pass the salt?\"\nLiteral meaning (semantics) : The speaker is asking about the listener's ability to \npass the salt.\nPragmatic meaning : In everyday conversation, the speaker is not asking about the \nlistener’s ability but is actually making a polite request for the salt to be passed.\n2. Speech Acts:\nApology : \"I’m sorry for being late.\"\noThe sentence is not just a statement of fact but also the act of apologizing.\nRequest : \"Could you please open the window?\"\noEven though it’s framed as a question, pragmatically it functions as a polite \nrequest for the listener to open the window.\n3. Deixis (Context-Dependent Words):\nExample : \"I will meet you here tomorrow.\"\noThe meaning of \"I\", \"you\" , \"here\" , and \"tomorrow\"  can only be understood \nwith knowledge of who the speaker and listener are, the location of the \nconversation, and the current date.\n4. Implicature:\nExample :\no\"It’s really noisy in here.\"\nThe literal meaning is simply an observation about the noise level.\n\n--- Page 15 ---\nThe pragmatic meaning  might be that the speaker is indirectly \nsuggesting to move to a quieter place.\nExample :\no\"Do you know what time it is?\"\nThe literal meaning is asking whether the listener knows the time.\nPragmatically, it’s likely a polite way of asking for the time.\nPragmatics vs. Semantics:\nSemantics : Deals with the literal meaning of words and sentences.\noExample: \"John is tall\"  simply means that John has a tall stature.\nPragmatics : Takes context and intention into account, providing an understanding \nbeyond the literal meaning.\noExample: \"John is tall\" could imply that John is taller than expected or that \nJohn should reach something high, depending on context.\nPragmatics in Real Life:\n1.Indirect Requests :\noExample :\nSentence: \"The trash is full.\"\nLiteral meaning : A statement about the trash.\nPragmatic meaning : This could be an indirect request for someone to \ntake out the trash.\n2.Politeness :\noExample :\nSentence: \"Could you close the door?\"\nLiteral meaning : A question about the listener’s ability to close the \ndoor.\nPragmatic meaning : It’s actually a polite request for the listener to \nclose the door, not a question about capability.\n3.Sarcasm :\noExample :\nSentence: \"Oh, great! Another traffic jam.\"\nLiteral meaning : The speaker is expressing excitement.\nPragmatic meaning : The speaker is actually expressing frustration, \nusing sarcasm to convey the opposite of what the words literally mean.\nQuestions and Answers on Pragmatics:\nQuestion 1: What is the pragmatic meaning of the sentence \"Can you pass the salt?\"  \nwhen spoken at a dinner table?\n1.A question about the listener’s ability to pass the salt.\n2.A polite request to pass the salt.\nAnswer : Option 2, a polite request to pass the salt.\nQuestion 2: Which of the following is an example of implicature?\n1.\"Could you open the door?\"  (Literal request for opening the door)\n\n--- Page 16 ---\n2.\"It's really hot in here.\"  (Implying that the speaker wants the listener to open a \nwindow)\nAnswer : Option 2. The speaker is implying that they want the listener to do something (open \na window) without directly stating it.\nPragmatics in NLP:\nIn Natural Language Processing (NLP) , understanding pragmatics is important for tasks \nlike:\nDialogue systems : Virtual assistants need to understand not only what is said but also \nthe context and intention behind it.\nSentiment analysis : Pragmatic understanding helps in detecting sarcasm or politeness \nin a sentence.\nContextual understanding : Chatbots and voice assistants use pragmatics to handle \nuser queries based on the context of the conversation.\nRole of Probability in Language Models\nIn Natural Language Processing (NLP), probability  plays a central role in building language \nmodels . A language model is a statistical tool that helps predict the likelihood of a sequence \nof words, and it relies on probability to make these predictions. By understanding the \nprobabilistic relationships between words and phrases, language models can generate, \nrecognize, and interpret natural language text.\nThe role of probability in language models can be broken down into the following key \naspects:\n1. Predicting the Next Word\nA fundamental application of probability in language models is predicting the likelihood of \nthe next word in a sequence, given the previous words. This is known as next-word \nprediction  or conditional probability .\nExample:\nSuppose you have the partial sentence:\n\"The dog is...\"\nA language model might assign the following probabilities to the next word:\nThe word \"barking\"  has the highest probability based on the context, so the model is most \nlikely to predict it as the next word.\n2. Language Modeling\n\n--- Page 17 ---\nLanguage models  are used to estimate the probability distribution over sequences of words. \nThese models help assess how likely a sentence is by calculating the joint probability of all \nthe words in a sentence. This is useful in applications such as machine translation, speech \nrecognition, and text generation.\nThe n-gram  approach uses probability to predict the likelihood of word sequences based on \nhow often they occur together in large text corpora.\n3. Handling Ambiguity\nIn many cases, a sentence can be interpreted in multiple ways, especially when it contains \nambiguous words. Probability helps resolve this ambiguity by calculating which \ninterpretation is more likely based on context.\nExample:\nSentence: \"I saw the man with the telescope.\"\noOne interpretation: I used the telescope to see the man.\noAnother interpretation: The man I saw had a telescope.\nA language model will calculate the probability of each interpretation based on the \nsurrounding context and choose the one with the higher probability.\n4. Smoothing Techniques\nIn practice, some word sequences may not occur frequently or may not exist at all in the \ntraining data, resulting in a probability of zero. Smoothing  techniques are used to handle \nthese cases and assign a small probability to unseen word sequences.\nAdditive Smoothing (Laplace Smoothing) : Adds a small constant to all possible \nword sequences to ensure none have zero probability.\nBackoff and Interpolation: These techniques combine n-grams of different lengths. If a \nhigher-order n-gram has zero probability, a lower-order n-gram (such as a bigram or \nunigram) is used to estimate the probability.\n5. Perplexity\nPerplexity  is a metric used to evaluate the performance of a language model. It measures \nhow well a probabilistic model predicts a sample. A lower perplexity score indicates that the \nmodel is better at predicting the probability distribution of the words in the text.\n\n--- Page 18 ---\nIn essence, perplexity quantifies how uncertain a model is when predicting the next word in a \nsentence. Better models have lower perplexity scores.\n6. Bayesian Inference in Language Models\nSome language models, such as Hidden Markov Models (HMMs)  and Latent Dirichlet \nAllocation (LDA) , use Bayesian inference  to predict hidden variables (such as word \ncategories or topics). Probability helps these models infer the underlying structure of text.\nFor example, in part-of-speech tagging , an HMM uses the probabilities of transitions \nbetween parts of speech and the likelihood of observing specific words to infer the most \nprobable part of speech for each word in a sentence.\nApplications of Probability in Language Models\n1.Text Generation : Probabilistic models can generate coherent sentences by predicting \nthe next word based on the previous ones. This is used in AI applications like GPT \n(Generative Pre-trained Transformer) models.\n2.Speech Recognition : By modeling the probability of word sequences, language \nmodels help in transcribing speech into text, making sure the most likely sentence is \nselected based on context.\n3.Machine Translation : Probabilistic language models translate sentences by \nestimating the likelihood of word sequences in both the source and target languages.\n4.Autocorrect and Predictive Text : Probabilistic models predict the most likely next \nword a user intends to type or suggest corrections for mistyped words based on their \nlikelihood in context.\nConditional Probability\nConditional probability  refers to the probability of an event occurring given that another \nevent has already occurred. It's a way of refining the probability of an event based on new \ninformation.\nMathematically, if A and B are two events, the conditional probability of A happening given \nthat B has occurred is denoted as:\nThe idea is that you're adjusting the probability of A by factoring in the occurrence of B.\nExample 1: Drawing Cards from a Deck\n\n--- Page 19 ---\nConsider a standard deck of 52 cards. Let’s calculate the conditional probability of drawing \nan Ace (event A) given that the card drawn is a Spade (event B).\nThere are 52 cards in the deck, and 13 of them are Spades.\nOut of the 13 Spades, only 1 is an Ace.\n\n\n--- Page 20 ---\nThis allows the doctor to calculate the likelihood that the person has the disease based on a \npositive test result, factoring in the accuracy and false-positive rate of the test.\nConditional Probability\nProblem 1\nYou toss a fair coin three times:\na.What is the probability of three heads, HHHHHH?\nb.What is the probability that you observe exactly one heads?\nc.Given that you have observed at least  one heads, what is the probability that you \nobserve at least two heads?\n\n--- Page 21 ---\nProblem 2\n\n\n--- Page 22 ---\nProblem 3\nIn my town, it's rainy one third of the days. Given that it is rainy, there will be heavy traffic \nwith probability 1/2, and given that it is not rainy, there will be heavy traffic with \nprobability 1/4. If it's rainy and there is heavy traffic, I arrive late for work with \nprobability 1/2. On the other hand, the probability of being late is reduced to 1/8 if it is not \nrainy and there is no heavy traffic. In other situations (rainy and no traffic, not rainy and \ntraffic) the probability of being late is 0.25. You pick a random day.\na.What is the probability that it's not raining and there is heavy traffic and I am not late?\nb.What is the probability that I am late?\nc.Given that I arrived late at work, what is the probability that it rained that day?\nLet R be the event that it's rainy, T be the event that there is heavy traffic, and L be the event \nthat I am late for work. As it is seen from the problem statement, we are given conditional \nprobabilities in a chain format. Thus, it is useful to draw a tree diagram. Figure 1.27 shows a \ntree diagram for this problem. In this figure, each leaf in the tree corresponds to a single \noutcome in the sample space. We can calculate the probabilities of each outcome in the \nsample space by multiplying the probabilities on the edges of the tree that lead to the \ncorresponding outcome.\n\n--- Page 24 ---\nJoint probability\nJoint Probability in NLP\nIn the field of Natural Language Processing (NLP) , joint probability is often used to model \nthe likelihood of word sequences, which is fundamental to tasks like:\n\n--- Page 25 ---\nLanguage modeling .\nSpeech recognition .\nMachine translation .\nJoint probability helps in determining how likely a sequence of words is in a particular \nlanguage, and it serves as the basis for making predictions about what words might come next \nin a sentence.\nIn Natural Language Processing (NLP) , Bayesian inference is commonly applied in spam \nfiltering , where we want to classify an email as spam or not based on its content. Words in \nthe email serve as evidence, and we calculate the probability that an email is spam given the \npresence of certain words.\nLet’s say the word \"free\"  appears in an email. Based on previous data:\n80% of emails containing the word \"free\"  are spam.\nOnly 20% of all emails are spam (this is our prior probability, P(spam)=0.2).\nThe word \"free\"  appears in 40% of all emails (spam and non-spam combined).\n\n--- Page 26 ---\nTokenization in NLP\nTokenization  is the process of splitting a sequence of text (like a sentence or a document) \ninto smaller pieces, called tokens . These tokens can be words, subwords, or characters, \ndepending on the level of tokenization. Tokenization is often the first step in Natural \nLanguage Processing (NLP) tasks like text analysis, machine translation, and information \nretrieval.\nIn simpler terms, tokenization  breaks down a stream of text into meaningful units that can be \nused for further processing, such as parsing or understanding the structure and meaning of the \ntext.\nTypes of Tokenization\n1.Word Tokenization :\noThe most common form of tokenization, where a text is split into words.\noFor example, given the sentence:\n\"The cat sat on the mat.\"\nAfter tokenization: \"The\",\"cat\",\"sat\",\"on\",\"the\",\"mat\",\".\"\nHere, each word is treated as a token. Special characters like punctuation (e.g., the \nperiod in this case) can either be treated as separate tokens or ignored, depending on \nthe tokenization approach.\nCommon Libraries for Word Tokenization:\nNLTK : nltk.word_tokenize()\nspaCy : spacy.tokenizer()\n\n--- Page 27 ---\n2.Subword Tokenization :\nThis breaks words into smaller units when dealing with languages or datasets where \ncertain words are rare or unknown.\nFor example, Byte Pair Encoding (BPE)  or WordPiece  is used in models like \nBERT. A word like \"running\"  could be split into \"run\", \"##ning\".\nThis helps in reducing the vocabulary size and handling unknown or rare words.\n3.Character Tokenization :\nHere, the text is split into individual characters. This method is useful in tasks like \nspeech recognition or handling languages like Chinese, where words may not be \nclearly separated by spaces.\nExample: \"Hello\" becomes \"H\",\"e\",\"l\",\"l\",\"o\"\n4.Sentence Tokenization :\n1.This splits text into sentences. It is useful in certain NLP tasks where the focus is on \nsentence-level understanding rather than word-level.\n2.Example: \"I love programming. It's fun!\" becomes \"Iloveprogramming.\",\"It′sfun!\"\nLibraries:\nNLTK : nltk.sent_tokenize()\nWhy Tokenization is Important?\n1.Preprocessing Step : In NLP, algorithms generally work with structured inputs. \nTokenization helps in breaking down the raw text into pieces that can be fed into \nmodels.\n2.Feature Extraction : Tokens serve as the basic features for many NLP tasks, such as \ntext classification, sentiment analysis, and machine translation. Without tokenization, \nthe model would struggle to interpret the structure of the input text.\n3.Efficiency : Tokenization reduces the complexity of text and allows models to focus \non smaller, more meaningful units, improving both the efficiency and accuracy of \nNLP tasks.\nChallenges in Tokenization\n1.Ambiguity : In some languages or cases, it's difficult to define clear token boundaries. \nFor instance, in \"Let's go\", should \"Let's\" be tokenized into \"Let\" and \"'s\"?\n2.Handling Punctuation : Deciding whether punctuation marks should be treated as \nseparate tokens or merged with adjacent words.\n3.Multilingual Tokenization : Tokenization rules vary across languages. For example, \nChinese text doesn’t have spaces between words, so segmenting text into words \n(tokenization) becomes more complex.\n4.Dealing with Compound Words : Languages like German have compound words \n(e.g., \"Donaudampfschiffahrtsgesellschaft\"), and determining where to split such \nwords can be tricky.\nExample of Tokenization in NLP\nConsider a sentence:\n\n--- Page 28 ---\nInput Sentence : \"She’s a great teacher.\"\nWord Tokenization:\nAfter tokenization: \"She\",\"′s\",\"a\",\"great\",\"teacher\",\".\"\nHere, the contraction \"She’s\"  is split into two tokens: \"She\" and \"'s\".\nTokenization in Modern NLP Models\nIn modern NLP models like BERT , GPT , and T5, subword tokenization methods like \nWordPiece  or Byte Pair Encoding (BPE)  are used. These methods break words into \nsubword units, which allows the models to handle rare and unseen words more efficiently. \nFor example:\nThe word \"unhappiness\" might be tokenized as: \"un\", \"##happi\", \"##ness\".\nThis approach helps in reducing the model's vocabulary size and improves the generalization \nof unseen words.\n5.Whitespace Tokenization\nThis is the simplest form of tokenization that splits the text based solely on spaces \n(whitespace) without considering punctuation.\nExample:\nInput Sentence :\n\"She loves programming in Python!\"\nTokenized Output :\n\"She\",\"loves\",\"programming\",\"in\",\"Python!\"\nThe punctuation is kept attached to the word (e.g., \"Python!\").\nUse Case:\nUseful when punctuation marks are meaningful or when simplicity is required.\n6.Regex Tokenization\nRegular expressions (regex)  can be used for highly customized tokenization, where specific \npatterns in the text are used to split tokens. Regex tokenization allows for control over what is \nconsidered a token.\nExample:\nInput Sentence :\n\"I paid $10 for 2 apples.\"\nRegex Tokenizer  (splitting by non-alphabetic characters):\nr'\\W+'\nTokenized Output :\n\"I\",\"paid\",\"10\",\"for\",\"2\",\"apples\"\nHere, numbers are treated as tokens, and special characters like \"$\" are removed.\nUse Case:\nUsed in domain-specific tasks where traditional tokenization may not be sufficient, \nlike extracting specific patterns from text (e.g., phone numbers, dates, etc.).\n7.Rule-Based Tokenization\nIn rule-based tokenization , predefined linguistic rules are used to split text into tokens. This \nmethod is useful for languages that do not use spaces between words (e.g., Chinese, \n\n--- Page 29 ---\nJapanese), or when more complex word boundaries need to be detected (e.g., contractions, \npossessives).\nExample:\nInput Sentence :\n\"Let’s go!\"\nTokenized Output :\n\"Let\",\"′s\",\"go\",\"!\"\nHere, the tokenization splits the contraction \"Let’s\" into two tokens: \"Let\" and \"'s\".\nUse Case:\nLanguages where word boundaries are not clearly defined by spaces, such as Chinese, \nJapanese, Thai , etc.\n8.Morpheme-Based Tokenization (Morphological Tokenization)\nMorpheme-based tokenization involves splitting words into their morphemes, which are the \nsmallest grammatical units that carry meaning. This is particularly useful for morphologically \nrich languages (e.g., Turkish, Finnish) where a word may have many suffixes or prefixes.\nExample:\nInput Word :\n\"unbelievable\"\nTokenized Output :\n\"un\",\"believe\",\"able\"\nHere, the word \"unbelievable\" is split into its morphemes: the prefix \"un\", the root \n\"believe\", and the suffix \"able\".\nUse Case:\nApplied in morphological analysis to study the structure of words.\nIntroduction to Morphology\nMorphology  is the branch of linguistics that studies the structure and formation of words. It \nfocuses on how words are built from smaller meaningful units called morphemes . \nMorphemes are the smallest grammatical units in a language that carry meaning.\nMorphology plays a significant role in natural language processing (NLP) because \nunderstanding the internal structure of words helps computers process and interpret language \nmore effectively. Morphology helps in various NLP tasks such as tokenization, part-of-\nspeech tagging, lemmatization, and more.\nKey Concepts in Morphology\n1.Morphemes :\noA morpheme is the smallest unit of meaning in a word.\noMorphemes can be divided into two categories:\nFree Morphemes : These can stand alone as independent words. \nExample: \"book\" , \"run\" .\nBound Morphemes : These cannot stand alone and must be attached to \na free morpheme. Example: \"-ed\"  (past tense), \"-s\" (plural).\n2.Types of Morphology :\n\n--- Page 30 ---\noInflectional Morphology : Deals with the modification of words to express \ndifferent grammatical categories such as tense, case, mood, or number. It does \nnot change the word's core meaning or part of speech.\nExample: \"run\"  → \"runs\" , \"run\"  → \"running\" .\noDerivational Morphology : Involves creating new words by adding prefixes \nor suffixes to existing words, often changing their meaning or part of speech.\nExample: \"happy\"  → \"unhappy\" , \"teach\"  → \"teacher\" .\n3.Root, Stem, and Affixes :\nRoot : The core part of a word that contains its basic meaning.\noExample: In \"unhappiness\" , the root is \"happy\" .\nStem : The form of the word that can take on inflectional morphemes.\noExample: In \"working\" , the stem is \"work\" .\nAffix : A morpheme that is attached to a root or stem to modify its meaning. Affixes \nare either:\noPrefix : Comes before the root (e.g., \"un-\"  in \"unhappy\" ).\noSuffix : Comes after the root (e.g., \"-ness\"  in \"happiness\" ).\nTypes of Morphemes\n1.Free Morphemes :\noThese are words that can function independently in a sentence.\noExample: \"dog\" , \"book\" , \"run\" .\n2.Bound Morphemes :\noThese must be attached to another morpheme to convey meaning.\noExample: \"-s\" (plural), \"-ed\"  (past tense).\nInflectional Morphology\nInflectional morphology  involves the modification of words to express different \ngrammatical features without changing the word’s core meaning or its part of speech.\nCommon Inflectional Morphemes:\nPlural : \"-s\" (e.g., \"dogs\" )\nPast Tense : \"-ed\"  (e.g., \"played\" )\nPresent Participle : \"-ing\"  (e.g., \"playing\" )\nComparative : \"-er\"  (e.g., \"faster\" )\nSuperlative : \"-est\"  (e.g., \"fastest\" )\nExample of Inflectional Morphology:\n\"play\"  → \"playing\"  (inflected for present participle)\n\"play\"  → \"played\"  (inflected for past tense)\nDerivational Morphology\nDerivational morphology  creates new words by adding affixes to a base word, often \nchanging its meaning or part of speech.\nExample of Derivational Morphology:\n\"happy\"  → \"unhappy\"  (prefix \"un-\"  changes the meaning to the opposite)\n\"teach\"  → \"teacher\"  (suffix \"-er\"  changes the word from a verb to a noun)\n\"create\"  → \"creation\"  (suffix \"-tion\"  changes the verb into a noun)\n\n--- Page 31 ---\nDerivational morphology often results in a more significant change than inflectional \nmorphology, as it can lead to the formation of completely new words.\nMorphological Parsing\nMorphological parsing  involves breaking down a word into its constituent morphemes. This \nis useful in various NLP tasks, especially in languages with rich morphology.\nExample of Morphological Parsing:\nFor the word \"unhappiness\" :\nun-: Prefix (negative)\nhappy : Root (basic meaning of the word)\n-ness : Suffix (turns the adjective into a noun)\nThus, \"unhappiness\"  is parsed as \"un−\",\"happy\",\"−ness\".\nImportance of Morphology in NLP\n1.Text Preprocessing : Morphological analysis helps in stemming and lemmatization, \nwhich are important preprocessing steps in NLP to reduce words to their base forms.\n2.Machine Translation : Understanding morphological structures helps machines \ntranslate languages more accurately, especially for morphologically rich languages \nlike Turkish or Finnish.\n3.Speech Recognition : Morphology helps in improving speech-to-text systems by \nunderstanding the variations in word forms.\n4.Named Entity Recognition (NER) : Recognizing entities such as person names, \nlocations, and organizations benefit from morphology, especially in morphologically \ncomplex languages.\nStemming\nStemming  is a natural language processing (NLP) technique that reduces a word to its stem  \nor base form, typically by stripping affixes (such as prefixes and suffixes). Unlike \nlemmatization , which reduces words to their dictionary form (lemma), stemming is a more \nrule-based and heuristic-driven process that often leads to non-lexical stems, meaning the \nresulting stem might not be a valid word.\nThe goal of stemming is to group together different forms of a word so that they can be \ntreated as the same item in tasks like information retrieval , search engines , or text mining .\nHow Stemming Works\nStemming works by applying rules to remove known suffixes and prefixes from words. The \nresulting word, called the stem , may not always be a valid dictionary word but captures the \ncore meaning of the original word.\nFor example:\nWord : \"running\"\nStem : \"run\"\nCommon stemming rules in English involve removing suffixes like -ing, -ed, -ly, -es, etc.\nTypes of Stemming Algorithms\n\n--- Page 32 ---\nThere are several types of stemming algorithms, with the Porter Stemmer  being one of the \nmost widely used. Other popular algorithms include the Snowball Stemmer  and the \nLancaster Stemmer .\n1.Porter Stemmer :\noOne of the oldest and most widely used stemming algorithms.\noIt applies a series of rules and transformations to remove common suffixes.\n2.Snowball Stemmer :\noAn improvement on the Porter Stemmer, more flexible and easier to modify \nfor different languages.\n3.Lancaster Stemmer :\noA more aggressive stemming algorithm, which can result in very short stems, \noften leading to over-stemming.\nExample of Stemming\nLet’s apply stemming to a few different words:\n1.Word : \"playing\"\noStem : \"play\"\n2.Word : \"played\"\noStem : \"play\"\n3.Word : \"player\"\noStem : \"play\"\n4.Word : \"happily\"\noStem : \"happi\"\nHere, the word \"play\"  remains the same after stemming, but the word \"happily\"  is reduced to \n\"happi\" . While \"happi\"  is not a valid word, it still captures the core meaning of the original \nword.\nStemming vs. Lemmatization\nStemming :\noStrips affixes according to rules.\noOften results in non-lexical forms (e.g., \"happi\"  instead of \"happy\" ).\noComputationally cheaper and faster than lemmatization.\nLemmatization :\noReduces words to their base form (lemma) using a dictionary.\noProduces valid dictionary words (e.g., \"better\"  → \"good\" ).\noRequires more computational resources than stemming.\nApplications of Stemming\n1.Search Engines :\n\n--- Page 33 ---\noSearch engines use stemming to treat different forms of a word (e.g., \"run\" , \n\"running\" , \"runner\" ) as equivalent. This ensures that a query for \"run\"  will \nreturn documents containing \"running\"  and \"runner\"  as well.\n2.Information Retrieval :\noIn information retrieval systems (such as databases and document searches), \nstemming helps improve the recall of relevant documents by matching word \nvariations.\n3.Text Classification :\noStemming helps in reducing the dimensionality of textual data by \nconsolidating multiple forms of the same word into a single representation.\n4.Text Mining :\noIn text mining tasks like sentiment analysis or topic modeling, stemming helps \ngroup different word forms, leading to more concise and generalizable results.\nQ1: Apply stemming to the following words: \"jumping\" , \"happily\" , \"studies\" , and \"players\" .\nAnswer :\n\"jumping\"  → \"jump\"\n\"happily\"  → \"happi\"\n\"studies\"  → \"studi\"\n\"players\"  → \"player\"\nPorter Stemmer Algorithm\nThe Porter Stemmer Algorithm  is one of the most widely used stemming algorithms in \nnatural language processing (NLP). Developed by Martin Porter in 1980, it applies a series of \nheuristic rules to strip common suffixes from English words, thus reducing them to their \n\"stem\" form. The goal is to remove morphological affixes and normalize words to their base \nform for tasks like search engines, information retrieval, and text mining.\nPorter Stemmer Algorithm Steps and Rules\nThe algorithm consists of five main steps, each containing a series of rules that are applied \nsequentially to a word. These rules focus on stripping suffixes, such as -ing, -ed, -ly, etc., to \nachieve the stem. The rules are based on conditions about the structure of the word, \nparticularly its measure  (or \"m-value\"), which helps in determining whether the word is \nmodified or not.\nMeasure (m-value)\nThe measure (m) is defined as the number of VC sequences  (vowel-consonant) in the word. \nFor example, in the word \"happy\" , the sequence is VC, so the measure (m) = 1.\nStep-by-Step Breakdown of the Porter Stemmer Algorithm\nStep 1: Removing Plural and Past Participle Suffixes\nStep 1a : Remove the plural suffix \"-s\".\noIf the word ends in \"sses\" , replace it with \"ss\".\noIf the word ends in \"ies\" , replace it with \"i\".\noIf the word ends in \"ss\", leave it unchanged.\noIf the word ends in \"s\", remove it (but only if there is a vowel in the word).\nExample :\n\n--- Page 34 ---\n\"caresses\"  → \"caress\"\n\"ponies\"  → \"poni\"\n\"cats\"  → \"cat\"\nStep 1b : Remove \"-ed\"  or \"-ing\"  if the word contains a vowel before the suffix.\noReplace \"-ed\"  with the base form.\noReplace \"-ing\"  with the base form.\nExample :\n\"hoped\"  → \"hope\"\n\"dancing\"  → \"danc\"\nAfter removing \"-ed\"  or \"-ing\" , if the resultant word ends with:\n\"at\", \"bl\", \"iz\", add an \"e\".\nDouble the last consonant if the word ends in a short vowel + consonant (e.g., \"hop\"  \n→ \"hopp\" ).\nIf the word ends in a consonant and a vowel followed by a consonant and has measure \n= 1, add \"e\" (e.g., \"plan\"  → \"plane\" ).\nStep 2: Replace Double Suffixes\nReplace common double suffixes like \"-ational\" , \"-izer\" , and \"-fulness\"  with shorter \nforms.\nRules :\n\"ational\"  → \"ate\"\n\"izer\"  → \"ize\"\nExample :\n\"relational\"  → \"relate\"\n\"optimizer\"  → \"optimize\"\nStep 3: Replace Other Common Suffixes\nReplace suffixes like \"-icate\" , \"-ful\" , \"-ness\" , and \"-ness\"  with simpler forms.\nRules :\n\"icate\"  → \"ic\"\n\"ness\"  → Remove entirely.\nExample :\n\"fortunate\"  → \"fortun\"\n\"goodness\"  → \"good\"\nStep 4: Remove Suffixes Based on Measure (m > 1)\nRemove suffixes like \"-ant\" , \"-ence\" , \"-ment\"  if the remaining word has measure (m) \n> 1.\nRules :\nIf the word ends with \"ement\" , \"ant\" , \"ence\" , remove them if the word has a measure \ngreater than 1.\nExample :\n\"dependent\"  → \"depend\"\nStep 5: Remove the Final \"-e\"  (if measure > 1)\nRemove a trailing \"-e\" if the word's measure (m) > 1. If m = 1 and the word ends with \n\"-e\", remove it unless the word is \"short\".\nExample :\n\n--- Page 35 ---\n\"probate\"  → \"probat\"\n\"rate\"  → \"rat\"\nExample of the Porter Stemmer in Action\nLet’s break down the Porter Stemmer’s application on the word \"relational\" :\n1.Step 1 : No changes in plural or past participle forms.\n2.Step 2 : Replace \"ational\"  with \"ate\" .\noResult : \"relate\"\nQ1) Apply the Porter Stemmer to the word \"happiness\" .\nAnswer :\nStep 1 : No changes.\nStep 2 : Remove \"-ness\" .\noResult : \"happi\"",
    "preview": "--- Page 1 ---\nModule 1: Natural Language Processing\nComponents - Basics of Linguistics and Probability and Statistics – Words-\nTokenization-Morphology:\nInflectional Morphology - Derivational Morphology. Finite-State Morphological \nParsing - Porter Stemmer.\nI. Introduction\nLanguage is a method of communication with the help of which we can speak, read and \nwrite. For example, we think, we make decisions, plans and more in natural language; \nprecisely, in words. However, the big question that con"
}